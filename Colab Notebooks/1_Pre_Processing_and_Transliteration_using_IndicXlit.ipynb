{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "def preprocess_hindi_text(text):\n",
        "    # Remove URLs\n",
        "    text = re.sub(r'http\\S+', '', text)\n",
        "\n",
        "    # Remove hashtags and the text following them\n",
        "    text = re.sub(r'#\\S+', '', text)\n",
        "\n",
        "    # Remove mentions/tags like @username\n",
        "    text = re.sub(r'@[A-Za-z0-9_]+', '', text)\n",
        "\n",
        "    # Remove numbers\n",
        "    text = re.sub(r'\\d+', '', text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = re.sub(r'[^\\w\\s]', '', text)\n",
        "\n",
        "    # Additional cleanup: Remove multiple spaces and trim leading/trailing spaces\n",
        "    text = re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "    return text\n",
        "\n",
        "# Read your dataset (replace 'your_dataset.csv' with your actual dataset file)\n",
        "dataset = pd.read_excel('constraint_Hindi_Train.xlsx')\n",
        "\n",
        "# Preprocess the Hindi text in the 'Post' column\n",
        "dataset['preprocessed_text'] = dataset['Post'].apply(preprocess_hindi_text)\n",
        "\n",
        "# Save the preprocessed dataset to a new Excel file\n",
        "dataset.to_excel('preprocessed_dataset.xlsx', index=False)\n",
        "\n",
        "# Display the first few rows of the preprocessed dataset\n",
        "print(dataset[['Post', 'preprocessed_text']].head())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ysj6hC5i67EP",
        "outputId": "039912c7-37ac-4818-fb01-a8e389b415fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                                Post  \\\n",
            "0  मेरे देश के हिन्दु बहुत निराले है। कुछ तो पक्क...   \n",
            "1  सरकार हमेशा से किसानों की कमाई को बढ़ाने के लि...   \n",
            "2  सुशांत ने जो बिजनेस डील 9 जून को की थी, वो डील...   \n",
            "3  @prabhav218 साले जेएनयू छाप कमिने लोग हिन्दुओं...   \n",
            "4  #unlock4guidelines - अनलॉक-4 के लिए गाइडलाइन्स...   \n",
            "\n",
            "                                   preprocessed_text  \n",
            "0  मर दश क हनद बहत नरल ह कछ त पकक रम भकत ह और कछ ...  \n",
            "1  सरकर हमश स कसन क कमई क बढन क लए नईनई सकम लत रह...  \n",
            "2  सशत न ज बजनस डल जन क क थ व डल दपश क सशत क हतय ...  \n",
            "3  सल जएनय छप कमन लग हनदओ क यह कहत ह क सवधन सबक ब...  \n",
            "4  अनलक क लए गइडलइनस जर सतबर स दशभर म मटर सव शर ह...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LSHVk7J9jKxK"
      },
      "source": [
        "## Installing the python library, that is wrapper around IndicXlit model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "18chFYcCfQhg",
        "outputId": "10d94b75-d7f1-421a-b9a4-d643febdcef7"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "GPU is available\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Check if GPU is available\n",
        "if tf.test.gpu_device_name():\n",
        "    print(\"GPU is available\")\n",
        "    # Explicitly set a GPU device if multiple GPUs are available\n",
        "    # For example, use the first GPU (device: 0)\n",
        "    # You can change the device number as needed\n",
        "    physical_devices = tf.config.list_physical_devices('GPU')\n",
        "    if physical_devices:\n",
        "        tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
        "else:\n",
        "    print(\"GPU is not available\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HMDCvP8uhNZ4",
        "outputId": "a1b6bb93-fb5a-44eb-f433-99ed67053a4e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ai4bharat-transliteration\n",
            "  Downloading ai4bharat_transliteration-1.1.3-py3-none-any.whl (32 kB)\n",
            "Collecting pydload (from ai4bharat-transliteration)\n",
            "  Downloading pydload-1.0.9-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: flask in /usr/local/lib/python3.10/dist-packages (from ai4bharat-transliteration) (2.2.5)\n",
            "Collecting flask-cors (from ai4bharat-transliteration)\n",
            "  Downloading Flask_Cors-4.0.0-py2.py3-none-any.whl (14 kB)\n",
            "Collecting gevent (from ai4bharat-transliteration)\n",
            "  Downloading gevent-23.9.1-cp310-cp310-manylinux_2_28_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m26.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sacremoses (from ai4bharat-transliteration)\n",
            "  Downloading sacremoses-0.0.53.tar.gz (880 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m880.6/880.6 kB\u001b[0m \u001b[31m43.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from ai4bharat-transliteration) (1.5.3)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from ai4bharat-transliteration) (4.66.1)\n",
            "Collecting ujson (from ai4bharat-transliteration)\n",
            "  Downloading ujson-5.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.9/53.9 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mock (from ai4bharat-transliteration)\n",
            "  Downloading mock-5.1.0-py3-none-any.whl (30 kB)\n",
            "Collecting tensorboardX (from ai4bharat-transliteration)\n",
            "  Downloading tensorboardX-2.6.2.2-py2.py3-none-any.whl (101 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.7/101.7 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (from ai4bharat-transliteration) (9.0.0)\n",
            "Collecting fairseq (from ai4bharat-transliteration)\n",
            "  Downloading fairseq-0.12.2.tar.gz (9.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.6/9.6 MB\u001b[0m \u001b[31m48.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting urduhack (from ai4bharat-transliteration)\n",
            "  Downloading urduhack-1.1.1-py3-none-any.whl (105 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m105.5/105.5 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting indic-nlp-library (from ai4bharat-transliteration)\n",
            "  Downloading indic_nlp_library-0.92-py3-none-any.whl (40 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.3/40.3 kB\u001b[0m \u001b[31m286.0 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cffi in /usr/local/lib/python3.10/dist-packages (from fairseq->ai4bharat-transliteration) (1.15.1)\n",
            "Requirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from fairseq->ai4bharat-transliteration) (3.0.2)\n",
            "Collecting hydra-core<1.1,>=1.0.7 (from fairseq->ai4bharat-transliteration)\n",
            "  Downloading hydra_core-1.0.7-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m123.8/123.8 kB\u001b[0m \u001b[31m13.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting omegaconf<2.1 (from fairseq->ai4bharat-transliteration)\n",
            "  Downloading omegaconf-2.0.6-py3-none-any.whl (36 kB)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.10/dist-packages (from fairseq->ai4bharat-transliteration) (2023.6.3)\n",
            "Collecting sacrebleu>=1.4.12 (from fairseq->ai4bharat-transliteration)\n",
            "  Downloading sacrebleu-2.3.1-py3-none-any.whl (118 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.9/118.9 kB\u001b[0m \u001b[31m14.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from fairseq->ai4bharat-transliteration) (2.0.1+cu118)\n",
            "Collecting bitarray (from fairseq->ai4bharat-transliteration)\n",
            "  Downloading bitarray-2.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (286 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m286.2/286.2 kB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: torchaudio>=0.8.0 in /usr/local/lib/python3.10/dist-packages (from fairseq->ai4bharat-transliteration) (2.0.2+cu118)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from fairseq->ai4bharat-transliteration) (1.23.5)\n",
            "Requirement already satisfied: Werkzeug>=2.2.2 in /usr/local/lib/python3.10/dist-packages (from flask->ai4bharat-transliteration) (2.3.7)\n",
            "Requirement already satisfied: Jinja2>=3.0 in /usr/local/lib/python3.10/dist-packages (from flask->ai4bharat-transliteration) (3.1.2)\n",
            "Requirement already satisfied: itsdangerous>=2.0 in /usr/local/lib/python3.10/dist-packages (from flask->ai4bharat-transliteration) (2.1.2)\n",
            "Requirement already satisfied: click>=8.0 in /usr/local/lib/python3.10/dist-packages (from flask->ai4bharat-transliteration) (8.1.7)\n",
            "Collecting zope.event (from gevent->ai4bharat-transliteration)\n",
            "  Downloading zope.event-5.0-py3-none-any.whl (6.8 kB)\n",
            "Collecting zope.interface (from gevent->ai4bharat-transliteration)\n",
            "  Downloading zope.interface-6.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (246 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m247.0/247.0 kB\u001b[0m \u001b[31m23.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: greenlet>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from gevent->ai4bharat-transliteration) (2.0.2)\n",
            "Collecting sphinx-argparse (from indic-nlp-library->ai4bharat-transliteration)\n",
            "  Downloading sphinx_argparse-0.4.0-py3-none-any.whl (12 kB)\n",
            "Collecting sphinx-rtd-theme (from indic-nlp-library->ai4bharat-transliteration)\n",
            "  Downloading sphinx_rtd_theme-1.3.0-py2.py3-none-any.whl (2.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m77.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting morfessor (from indic-nlp-library->ai4bharat-transliteration)\n",
            "  Downloading Morfessor-2.0.6-py3-none-any.whl (35 kB)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ai4bharat-transliteration) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->ai4bharat-transliteration) (2023.3.post1)\n",
            "Requirement already satisfied: progressbar2 in /usr/local/lib/python3.10/dist-packages (from pydload->ai4bharat-transliteration) (4.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from pydload->ai4bharat-transliteration) (2.31.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from sacremoses->ai4bharat-transliteration) (1.16.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from sacremoses->ai4bharat-transliteration) (1.3.2)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorboardX->ai4bharat-transliteration) (23.1)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX->ai4bharat-transliteration) (3.20.3)\n",
            "Collecting tf2crf (from urduhack->ai4bharat-transliteration)\n",
            "  Downloading tf2crf-0.1.33-py2.py3-none-any.whl (7.3 kB)\n",
            "Collecting tensorflow-datasets~=3.1 (from urduhack->ai4bharat-transliteration)\n",
            "  Downloading tensorflow_datasets-3.2.1-py3-none-any.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m94.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of urduhack to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting urduhack (from ai4bharat-transliteration)\n",
            "  Downloading urduhack-1.1.0-py3-none-any.whl (104 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m12.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading urduhack-1.0.3-py3-none-any.whl (97 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.0/98.0 kB\u001b[0m \u001b[31m11.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading urduhack-1.0.2-py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.5/96.5 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading urduhack-1.0.1-py3-none-any.whl (96 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m96.7/96.7 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading urduhack-1.0.0-py3-none-any.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.5/99.5 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading urduhack-0.3.4-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.9/81.9 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting transformers~=2.10 (from urduhack->ai4bharat-transliteration)\n",
            "  Downloading transformers-2.11.0-py3-none-any.whl (674 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m674.8/674.8 kB\u001b[0m \u001b[31m52.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urduhack (from ai4bharat-transliteration)\n",
            "  Downloading urduhack-0.3.3-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.3/82.3 kB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: pip is looking at multiple versions of urduhack to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading urduhack-0.3.2-py3-none-any.whl (81 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m81.6/81.6 kB\u001b[0m \u001b[31m9.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow~=2.2 in /usr/local/lib/python3.10/dist-packages (from urduhack->ai4bharat-transliteration) (2.13.0)\n",
            "  Downloading urduhack-0.3.1-py3-none-any.whl (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading urduhack-0.2.7-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/75.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting regex (from fairseq->ai4bharat-transliteration)\n",
            "  Downloading regex-2019.12.20.tar.gz (679 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m679.8/679.8 kB\u001b[0m \u001b[31m17.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting urduhack (from ai4bharat-transliteration)\n",
            "  Downloading urduhack-0.2.6-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.3/75.3 kB\u001b[0m \u001b[31m8.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading urduhack-0.2.5-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.3/71.3 kB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hINFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading urduhack-0.2.4-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.3/71.3 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading urduhack-0.2.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.2/71.2 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading urduhack-0.2.2-py3-none-any.whl (66 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.5/66.5 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading urduhack-0.2.1-py3-none-any.whl (65 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.0/66.0 kB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Downloading urduhack-0.1.4-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.9/62.9 kB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting antlr4-python3-runtime==4.8 (from hydra-core<1.1,>=1.0.7->fairseq->ai4bharat-transliteration)\n",
            "  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.4/112.4 kB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from Jinja2>=3.0->flask->ai4bharat-transliteration) (2.1.3)\n",
            "Requirement already satisfied: PyYAML>=5.1.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq->ai4bharat-transliteration) (6.0.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from omegaconf<2.1->fairseq->ai4bharat-transliteration) (4.5.0)\n",
            "Collecting portalocker (from sacrebleu>=1.4.12->fairseq->ai4bharat-transliteration)\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq->ai4bharat-transliteration) (0.9.0)\n",
            "Collecting colorama (from sacrebleu>=1.4.12->fairseq->ai4bharat-transliteration)\n",
            "  Downloading colorama-0.4.6-py2.py3-none-any.whl (25 kB)\n",
            "Requirement already satisfied: lxml in /usr/local/lib/python3.10/dist-packages (from sacrebleu>=1.4.12->fairseq->ai4bharat-transliteration) (4.9.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->fairseq->ai4bharat-transliteration) (3.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->fairseq->ai4bharat-transliteration) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->fairseq->ai4bharat-transliteration) (3.1)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->fairseq->ai4bharat-transliteration) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->fairseq->ai4bharat-transliteration) (3.27.4.1)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->fairseq->ai4bharat-transliteration) (16.0.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi->fairseq->ai4bharat-transliteration) (2.21)\n",
            "Requirement already satisfied: python-utils>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from progressbar2->pydload->ai4bharat-transliteration) (3.7.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->pydload->ai4bharat-transliteration) (3.2.0)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->pydload->ai4bharat-transliteration) (3.4)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->pydload->ai4bharat-transliteration) (2.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->pydload->ai4bharat-transliteration) (2023.7.22)\n",
            "Requirement already satisfied: sphinx>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx-argparse->indic-nlp-library->ai4bharat-transliteration) (5.0.2)\n",
            "Requirement already satisfied: docutils<0.19 in /usr/local/lib/python3.10/dist-packages (from sphinx-rtd-theme->indic-nlp-library->ai4bharat-transliteration) (0.18.1)\n",
            "Collecting sphinxcontrib-jquery<5,>=4 (from sphinx-rtd-theme->indic-nlp-library->ai4bharat-transliteration)\n",
            "  Downloading sphinxcontrib_jquery-4.1-py2.py3-none-any.whl (121 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.1/121.1 kB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from zope.event->gevent->ai4bharat-transliteration) (67.7.2)\n",
            "Requirement already satisfied: sphinxcontrib-applehelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->ai4bharat-transliteration) (1.0.7)\n",
            "Requirement already satisfied: sphinxcontrib-devhelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->ai4bharat-transliteration) (1.0.5)\n",
            "Requirement already satisfied: sphinxcontrib-jsmath in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->ai4bharat-transliteration) (1.0.1)\n",
            "Requirement already satisfied: sphinxcontrib-htmlhelp>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->ai4bharat-transliteration) (2.0.4)\n",
            "Requirement already satisfied: sphinxcontrib-serializinghtml>=1.1.5 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->ai4bharat-transliteration) (1.1.9)\n",
            "Requirement already satisfied: sphinxcontrib-qthelp in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->ai4bharat-transliteration) (1.0.6)\n",
            "Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->ai4bharat-transliteration) (2.16.1)\n",
            "Requirement already satisfied: snowballstemmer>=1.1 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->ai4bharat-transliteration) (2.2.0)\n",
            "Requirement already satisfied: babel>=1.3 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->ai4bharat-transliteration) (2.12.1)\n",
            "Requirement already satisfied: alabaster<0.8,>=0.7 in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->ai4bharat-transliteration) (0.7.13)\n",
            "Requirement already satisfied: imagesize in /usr/local/lib/python3.10/dist-packages (from sphinx>=1.2.0->sphinx-argparse->indic-nlp-library->ai4bharat-transliteration) (1.4.1)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->fairseq->ai4bharat-transliteration) (1.3.0)\n",
            "Building wheels for collected packages: fairseq, sacremoses, antlr4-python3-runtime\n",
            "  Building wheel for fairseq (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fairseq: filename=fairseq-0.12.2-cp310-cp310-linux_x86_64.whl size=11291797 sha256=e60d5ff686cca1c826278ab6c871848716fbab1fbfd40fe4ae5855b2e3019497\n",
            "  Stored in directory: /root/.cache/pip/wheels/e4/35/55/9c66f65ec7c83fd6fbc2b9502a0ac81b2448a1196159dacc32\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.53-py3-none-any.whl size=895241 sha256=a5770727c1d26eef4df5b5061b75001b066770cf1d6f9838e393fb99fd0361d9\n",
            "  Stored in directory: /root/.cache/pip/wheels/00/24/97/a2ea5324f36bc626e1ea0267f33db6aa80d157ee977e9e42fb\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141209 sha256=6912034abde85624a821a147ed6dc49d4037b18c0aa49c0d318ef9fa03d7bbf9\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/20/bd/e1477d664f22d99989fd28ee1a43d6633dddb5cb9e801350d5\n",
            "Successfully built fairseq sacremoses antlr4-python3-runtime\n",
            "Installing collected packages: morfessor, bitarray, antlr4-python3-runtime, zope.interface, zope.event, urduhack, ujson, tensorboardX, sacremoses, portalocker, omegaconf, mock, colorama, sacrebleu, hydra-core, gevent, pydload, flask-cors, sphinxcontrib-jquery, sphinx-rtd-theme, sphinx-argparse, indic-nlp-library, fairseq, ai4bharat-transliteration\n",
            "Successfully installed ai4bharat-transliteration-1.1.3 antlr4-python3-runtime-4.8 bitarray-2.8.1 colorama-0.4.6 fairseq-0.12.2 flask-cors-4.0.0 gevent-23.9.1 hydra-core-1.0.7 indic-nlp-library-0.92 mock-5.1.0 morfessor-2.0.6 omegaconf-2.0.6 portalocker-2.8.2 pydload-1.0.9 sacrebleu-2.3.1 sacremoses-0.0.53 sphinx-argparse-0.4.0 sphinx-rtd-theme-1.3.0 sphinxcontrib-jquery-4.1 tensorboardX-2.6.2.2 ujson-5.8.0 urduhack-0.1.4 zope.event-5.0 zope.interface-6.0\n"
          ]
        }
      ],
      "source": [
        "# installing library\n",
        "# for thorough documentation: https://pypi.org/project/ai4bharat-transliteration/\n",
        "!pip install ai4bharat-transliteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tp_6RHAJjBrJ"
      },
      "source": [
        "## Import the module for transliteration engine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hlqnZ8RrjI-7"
      },
      "outputs": [],
      "source": [
        "# model support the following languages : [as, bn, brx, gom, gu, hi, kn, ks, mai, ml, mni, mr, ne, or, pa, sa, sd, si, ta, te, ur]\n",
        "# importing ai4bharat transliteration module\n",
        "from ai4bharat.transliteration import XlitEngine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dnYV1YX-iy4A"
      },
      "source": [
        "## Using word Transliteration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uklMFr9zjmh4"
      },
      "source": [
        "- beam_width increases beam search size, resulting in improved accuracy but increases time/compute. (Default: 4)\n",
        "- topk returns only specified number of top results. (Default: 4)\n",
        "- rescore returns the reranked suggestions after using a dictionary. (Default: True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4blljLKI1fy7"
      },
      "source": [
        "#### En-Indic conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gLz7B4pxhPq4",
        "outputId": "21995184-8995-4975-9f8c-b2584099de58"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing Multilingual model for transliteration\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading dicts into RAM: 100%|██████████| 1/1 [00:06<00:00,  6.75s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'hi': ['how are यू']}\n"
          ]
        }
      ],
      "source": [
        "# intializing the en-indic multilingual model and dictionaries (if rerank option is True)\n",
        "e = XlitEngine(\"hi\", beam_width=10, rescore=True, src_script_type = \"en\")\n",
        "\n",
        "# transliterate word\n",
        "out = e.translit_word(\"How are you\", topk=1)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LXv2pg2S1lMz"
      },
      "source": [
        "#### Indic-En conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gEsUEQOP1aHk",
        "outputId": "58e93a06-f3b9-4b38-f247-4d1f0db6e223"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing Multilingual model for transliteration\n",
            "['bhaarat']\n",
            "['gujaraat', 'gujarat', 'goojarat', 'gujraat']\n"
          ]
        }
      ],
      "source": [
        "# intializing the indic-en multilingual model and dictionaries (if rerank option is True)\n",
        "e = XlitEngine( beam_width=4, rescore=False, src_script_type = \"indic\")\n",
        "\n",
        "# transliterate Hindi word\n",
        "out = e.translit_word(\"भारत\", 'hi', topk=1)\n",
        "print(out)\n",
        "\n",
        "# transliterate Gujarati word\n",
        "out = e.translit_word(\"ગુજરાત\", 'gu', topk=5)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHfnnSY-i42Z"
      },
      "source": [
        "## word Transliteration without rescoring"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XbdQL-Xd4CF1"
      },
      "source": [
        "#### En-Indic conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wlw5eJZWi1U8",
        "outputId": "28590e50-9fa8-47bc-a1c1-a192e6ea75f9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing Multilingual model for transliteration\n",
            "{'hi': ['ओने', 'ओन', 'ओनी', 'ओणे']}\n"
          ]
        }
      ],
      "source": [
        "e = XlitEngine(\"hi\", beam_width=4, rescore=False, src_script_type = \"en\")\n",
        "out = e.translit_word(\"one\", topk=5)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C1a5Byux4AR-"
      },
      "source": [
        "#### Indic-En conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bAr6kgre3594",
        "outputId": "70a4fce8-62eb-4587-c357-9e9d923e43da"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing Multilingual model for transliteration\n",
            "['bhaarat', 'bharat', 'bharath', 'bharata', 'bhaarut']\n"
          ]
        }
      ],
      "source": [
        "# intializing the indic-en multilingual model and dictionaries (if rerank option is True)\n",
        "e = XlitEngine( beam_width=10, rescore=False, src_script_type = \"indic\")\n",
        "\n",
        "# transliterate Hindi word\n",
        "out = e.translit_word(\"भारत\", 'hi', topk=5)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QxCdp1Y34O58"
      },
      "source": [
        "#### En-Indic conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GCCpEKTQj2Yy",
        "outputId": "52762374-7328-4189-c17e-e8cd98b3d65f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing Multilingual model for transliteration\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading dicts into RAM: 100%|██████████| 2/2 [00:20<00:00, 10.22s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'mr': '१०२ वणक्कम उलगम', 'te': '౧౦౨ వణక్కం ఉలగం'}\n"
          ]
        }
      ],
      "source": [
        "e = XlitEngine([\"te\", 'mr'], beam_width=10, src_script_type = \"en\")\n",
        "out = e.translit_sentence(\"102 VAnakkam ulagam\")\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7Zrj3lE4Pdo"
      },
      "source": [
        "#### Indic-En conversion"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kAhZCHqs4Fhn",
        "outputId": "094f1cfc-9e19-419a-d4cb-6466e9c334a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing Multilingual model for transliteration\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading dicts into RAM: 100%|██████████| 1/1 [00:00<00:00,  8.66it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "vanakkam ulagam\n"
          ]
        }
      ],
      "source": [
        "e = XlitEngine( beam_width=4, src_script_type = \"indic\")\n",
        "out = e.translit_sentence(\"వణక్కం ఉలగం\", 'te')\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SAbxWwFji7TS"
      },
      "source": [
        "## Using Multiple language Transliteration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0_CL02Lj284",
        "outputId": "829910b0-8b7a-4290-8278-b91dc75e4078"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing Multilingual model for transliteration\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\rLoading dicts into RAM:   0%|          | 0/2 [00:00<?, ?it/s]"
          ]
        }
      ],
      "source": [
        "# Pass list of languages for multile language transliteration\n",
        "e = XlitEngine([\"ta\", \"ml\"], beam_width=6, src_script_type = \"en\")\n",
        "# leave empty or use \"all\" to load all available languages\n",
        "# e = XlitEngine(\"all)\n",
        "\n",
        "out = e.translit_word(\"amma\", topk=3)\n",
        "print(out)\n",
        "\n",
        "out = e.translit_sentence(\"hello world\")\n",
        "print(out)\n",
        "\n",
        "## Specify language name to get only specific language result\n",
        "out = e.translit_word(\"amma\", topk=5)\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DypOGQxXi9SJ"
      },
      "source": [
        "## Transliteration for all available languages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7NqUeCKKj3bY",
        "outputId": "9ee80475-cfae-47a9-91c2-cd71668649a0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Initializing Multilingual model for transliteration\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading dicts into RAM:  14%|█▍        | 3/21 [00:06<00:36,  2.01s/it]"
          ]
        }
      ],
      "source": [
        "# loading all the language dictionaries would require 8-10 gb of space in RAM\n",
        "e = XlitEngine(beam_width=10, src_script_type = \"en\")\n",
        "out = e.translit_sentence(\"Hello World\")\n",
        "print(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EWrgeh6xAUVH"
      },
      "outputs": [],
      "source": [
        "!pip install tdqm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xe3Tb7ZJEPt8",
        "outputId": "4c4f719f-bda3-49e2-ad2c-edcf3f6d85bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initializing Multilingual model for transliteration\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Loading dicts into RAM: 100%|██████████| 1/1 [00:07<00:00,  7.65s/it]\n",
            "Progress: 178/178: 100%|██████████| 178/178 [2:34:09<00:00, 51.96s/it]\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from ai4bharat.transliteration import XlitEngine\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define batch size\n",
        "batch_size = 32  # You can adjust this value as needed\n",
        "\n",
        "# Initialize the transliteration engine for Hindi\n",
        "e = XlitEngine(\"hi\", beam_width=10, rescore=True, src_script_type=\"en\")\n",
        "\n",
        "# Load your dataset from your computer\n",
        "input_file_path = 'TWITTER+youtube(ML Project).csv'\n",
        "output_file_path = 'output_dataset.csv'\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv(input_file_path, encoding='latin1')\n",
        "\n",
        "# Create a new column for transliterated text\n",
        "df['transliterated_text'] = ''\n",
        "\n",
        "# Create a tqdm progress bar for iteration\n",
        "total_batches = (len(df) + batch_size - 1) // batch_size\n",
        "progress_bar = tqdm(total=total_batches, position=0, leave=True)\n",
        "\n",
        "# Iterate through the dataset in batches and transliterate\n",
        "for batch_start in range(0, len(df), batch_size):\n",
        "    batch_end = min(batch_start + batch_size, len(df))\n",
        "    batch_df = df.iloc[batch_start:batch_end]\n",
        "\n",
        "    for index, row in batch_df.iterrows():\n",
        "        input_text = row['Tweet']\n",
        "\n",
        "        # Check if input_text is a string, and convert it to string if not\n",
        "        if not isinstance(input_text, str):\n",
        "            input_text = str(input_text)\n",
        "\n",
        "        transliterated_text = e.translit_sentence(input_text)\n",
        "        df.at[index, 'transliterated_text'] = transliterated_text\n",
        "\n",
        "    # Update the progress bar\n",
        "    progress_bar.update(1)\n",
        "    progress_bar.set_description(f\"Progress: {progress_bar.n}/{total_batches}\")\n",
        "\n",
        "# Close the progress bar\n",
        "progress_bar.close()\n",
        "\n",
        "# Save the updated dataset to a new .csv file\n",
        "df.to_csv(output_file_path, index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JARzX5BLuObZ"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}